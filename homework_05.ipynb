{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework of Ch5. Text Classification\n",
    "----\n",
    "This is the homework of TU-ETP-AD1062 Machine Learning Fundamentals.\n",
    "\n",
    "For more information, please refer to:\n",
    "https://sites.google.com/view/tu-ad1062-mlfundamentals/\n",
    "\n",
    "> You do NOT have to build up from nothing, please try your best for the following parts:\n",
    "> - **Your task: HW5.2.2.**\n",
    "> - **Your task: HW5.2.3.**\n",
    "> - **Your task: HW5.3.1.**\n",
    "> - **Your task: HW5.3.2.**\n",
    "> - **Your task: HW5.3.3.**\n",
    "> - **Your task: HW5.4.**\n",
    "> \n",
    "> Please refer to the `demo_05.ipynb` If you have no idea about the API usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1. Import Packages\n",
    "----\n",
    "- Models construction:\n",
    "    - `keras`:\n",
    "        - `preprocessing.*`: text cleanup, text pre-processing, and text sequences tokenization before import into models\n",
    "        - `models.*`, `embeddings.*`, `layers.*`, and `optimizers.*`: For loading related components layers to constructing recurrent neural network (including both LSTM, GRU or the simplest version RNN)\n",
    "        - `utils.to_categorical`: For converting numerical labels into categorical labels\n",
    "- Performance evaluation:\n",
    "    - `sklearn.metrics.zero_one_loss`: Used for accuracy evaluation\n",
    "    - `sklearn.model_selection.train_test_split`: Divide your data into training and validation set for once, then feed into classifier by yourself, observing the score and confusion matrix\n",
    "    - `mlfund.plot.PlotMetric`: plot confusion matrix (provided by this repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mlfund.plot import PlotMetric\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2. Data pre-processing\n",
    "\n",
    "\n",
    "### HW5.2.1. Read Dataset from CSV files\n",
    "----\n",
    "The code snippet is used to read training and testing set from CSV files, then conduct text-preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "df_train = pd.read_csv(os.path.join('data', 'hw5', 'hw5.train.csv'))\n",
    "\n",
    "id_train = df_train['id']\n",
    "X_train = df_train['review']\n",
    "y_train = list(df_train['label'])\n",
    "\n",
    "\n",
    "# Testing set\n",
    "df_test = pd.read_csv(os.path.join('data', 'hw5', 'hw5.test.csv'))\n",
    "\n",
    "id_test = df_test['id']\n",
    "X_test = df_test['review']\n",
    "\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2.2. Tokenize Sentences to Word Lists\n",
    "---\n",
    "> **Your task: HW5.2.2.**  \n",
    "> Please follow the demo of **Demo 5.4.1.** in `demo_05.ipynb` to conduct sentence tokenization.  \n",
    "> For example, for the following sentence:\n",
    "> - `I'm great admirer Lon Chaney, screen writing movie work me.`\n",
    "> \n",
    "> The expected result is:  \n",
    "> - `['i'm', 'great', 'admirer', 'lon', 'chaney', 'screen', 'writing', 'movie', 'work', 'me']`\n",
    ">\n",
    "> You're expected to conduct sentence tokenization for the sentences stored in the following 2 lists:  \n",
    "> - `df_train['review']`\n",
    "> - `df_test['review']`\n",
    "> ---\n",
    "> For your conveniences of using the code snippets afterwards:\n",
    "> - Use `X_train_str` and `X_test_str` for storing the tokenized results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_str = (...Uncomment this line and finish your task! ... )\n",
    "# X_test_str = (...Uncomment this line and finish your task! ... )\n",
    "\n",
    "\n",
    "# The following snippet helps you to show the first 10 words for each tokenized results of the first 20 sentences in training set\n",
    "for i in range(0, 20):\n",
    "    print('len = %d, [\\'%s\\' ...]' % (len(X_train_str[i]), '\\', \\''.join(X_train_str[i][0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2.3. One-hot Encoding for the Top-K words\n",
    "----\n",
    "> **Your task: HW5.2.3.**  \n",
    "> Please refer to the **Demo 5.4.2** in `demo_05.ipynb`, finish the following tasks:\n",
    "> 1. Calculate the most frequently used words by training a `keras.preprocessing.text.Tokenizer` instance.  \n",
    "**You are expected to train the tokenizer with `X_train_str` Only, since you're not expected to have any knowledge about your testing data**\n",
    "> 2. Convert each sentences to one-hot encoding by the trained `keras.preprocessing.text.Tokenizer` instance in the above step, and\n",
    "> 3. Expand token list to the fixed length by `keras.preprocessing.sequence.pad_sequences`\n",
    ">\n",
    ">\n",
    "> You're also expected to adjust the `MAX_NUM_DICT_WORDS`, `MAX_SEQUENCE_LENGTH`, and `WORD_EMBED_DIMENSION`. The additional information for the 3 parameters are listed below:\n",
    "> 1. Only the top-`MAX_NUM_DICT_WORDS` will be reserved and convert to word vectors, whereas the other will be neglected\n",
    "> 2. Only the first-`MAX_SEQUENCE_LENGTH` words will be reserved for each review, whereas the words afterwords will be truncated.\n",
    "> 3. The word embedding dimension will be controlled by `WORD_EMBED_DIMENSION`:\n",
    ">    - If you use the `glove.6B.50d.txt` for word vectors converting, DO NOT modify this value\n",
    ">    - If you use the self-trained word embedding (i.e., `trainable=True` in Keras embedding layer), try to adjust for this value\n",
    "> ----\n",
    "> For your conveniences of using the code snippets afterwards:\n",
    "> - Use `tokenizer` as the `keras.preprocessing.text.Tokenizer` instance,\n",
    "> - Use `X_train` as the name of fixed-length one-hot encoded word index list of training set, and\n",
    "> - Use `X_test` as the name of fixed-length one-hot encoded word index list of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_DICT_WORDS     = 5000\n",
    "MAX_SEQUENCE_LENGTH    = 200\n",
    "WORD_EMBED_DIMENSION   = 50\n",
    "\n",
    "# tokenizer = (...Uncomment this line and finish your task! ... )\n",
    "\n",
    "# X_train = (...Uncomment this line and finish your task! ... )\n",
    "# X_test  = (...Uncomment this line and finish your task! ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional)** The following snippet helps you to examine the result of Top-K words tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(tokenizer.index_word)[:50]:\n",
    "    print('%d\\t => %s' % (key, tokenizer.index_word[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional)** The following snippet helps you to examine the one-hot encoded result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_train in X_train:\n",
    "    assert isinstance(x_train, np.ndarray) and len(x_train) == MAX_SEQUENCE_LENGTH, \"X_train should be an instance of numpy.ndarray, with shape[1] == MAX_SEQUENCE_LENGTH\"\n",
    "\n",
    "for i in range(0, 20):\n",
    "    print('%s' % (X_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2.4. Convert each Words by External Word2Vec Data\n",
    "---\n",
    "The code snippet here shows how to convert the words by external **GloVe** representations.\n",
    "\n",
    "- You're required to manually download the `glove.6B.50d.zip` from the Kaggle InClass competition > Data page (60.5MB), or:  \n",
    "https://powerbox-file.trendmicro-cloud.com/SFDC/external_shared/b98384991fe44745da75780232ec0961.php\n",
    "- After downloaded, decompress it and place the `glove.6B.50d.txt` under `data/hw5`\n",
    "- The word dictionary mapped from One-hot encoding index to word vectors would then be built based on the top-K tokenizer constructed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized words to vector\n",
    "word2vec_path = os.path.join('data', 'demo5', 'glove.6B.50d.txt')\n",
    "\n",
    "if not os.path.exists(word2vec_path):\n",
    "    raise FileNotFoundError('Please follow the instructions mentioned above to get glove.6B.50d.txt')\n",
    "\n",
    "embed_mat = np.zeros( (MAX_NUM_DICT_WORDS + 1, WORD_EMBED_DIMENSION ) )\n",
    "with codecs.open(word2vec_path, 'r', 'utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        tokens = line.rstrip(' \\r\\n').split(' ')\n",
    "        \n",
    "        word_key    = tokens[0]\n",
    "        word_vector = [float(i) for i in tokens[1:]]\n",
    "        \n",
    "        if word_key in tokenizer.word_index and tokenizer.word_index[word_key] < MAX_NUM_DICT_WORDS + 1:\n",
    "            embed_mat[ tokenizer.word_index[word_key], : ] = word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional)** The following snippet helps you to examine the word2vec result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word2vec_result = []\n",
    "for i in range(2,22):\n",
    "    dict_word2vec_result.append({\n",
    "        'word': tokenizer.index_word[i],\n",
    "        'vector': '%s' % embed_mat[i]\n",
    "    })\n",
    "    \n",
    "display(pd.DataFrame(dict_word2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3. Construct and Train Text Classification Model\n",
    "\n",
    "### HW 5.3.1. Adjust the Model\n",
    "----\n",
    "The code snippet shown below constructs a LSTM with following structure:\n",
    "1. Embedding Layer:\n",
    "    * `input_dim`: Maximum words appeared in the dictionary, which should be `MAX_NUM_DICT_WORDS + 1`\n",
    "    * `output_dim`: Word embedding dimension, which should be `WORD_EMBED_DIMENSION` set above\n",
    "    * `input_length`: Max sequence length, which should be `MAX_SEQUENCE_LENGTH` set above\n",
    "2. LSTM Layer:\n",
    "    * Unit size 64\n",
    "3. Fully-connected layer\n",
    "    * Unit size 256\n",
    "4. Drop-out layer\n",
    "5. Fully-connected layer with sigmoid activation\n",
    "\n",
    "> **Your task: HW5.3.1.**  \n",
    "> Build and adjust your own models by Keras framework, try to maximize the performance by adjust the model structures.\n",
    "> Some documents listed below might be useful:\n",
    "> - Embedding Layer: https://keras.io/layers/embeddings/#embedding\n",
    "> - LSTM Layer: https://keras.io/layers/recurrent/#lstm\n",
    "> - GRU Layer: https://keras.io/layers/recurrent/#gru\n",
    "> - Dense Layer (Fully-connected layer): https://keras.io/layers/core/#dense\n",
    ">\n",
    "> **Notice:** You can use either:\n",
    "> - RNN/LSTM/GRU model mentioned in Chapter 5. (Same as the code snippet), or\n",
    "> - CNN model mentioned in Chapter 4. (Please refer to **Demo 4.3.1.** in `demo04.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(MAX_NUM_DICT_WORDS + 1, WORD_EMBED_DIMENSION, weights=[embed_mat], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(LSTM(10))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3.2. Train the Model\n",
    "----\n",
    "The code snippet helps you to split the known, training data `X_train`, `y_train` into `X1`, `X2`, `y1`, `y2` for validation.\n",
    "\n",
    "> **Your task: HW5.3.2.**  \n",
    "> Adjust your `fit` process for training, including `batch_size` and `epochs` to meet your hardware conditions.\n",
    "> \n",
    "> For more details, see: https://keras.io/models/model/#fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_rnn()\n",
    "model.summary()\n",
    "\n",
    "X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "y1_categorical = to_categorical(y1)\n",
    "model.fit(X1, y1_categorical, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3.3. Performance Evaluation\n",
    "----\n",
    "> **Your task: HW5.3.3.**  \n",
    "> Check the zero-one-loss and confusion matrix to adjust the performance.\n",
    ">\n",
    "> **Notice:**\n",
    "> - In general, one should conduct cross-validation mentioned in Chapter 1.\n",
    "> - To save the time, you are allowed to conduct single train and test split in order to metigate the time consumption of performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_categorical_predict = model.predict(X2)\n",
    "y2_predict = np.argmax(y2_categorical_predict, axis=1)\n",
    "\n",
    "# Error rate\n",
    "err_01loss = zero_one_loss(y2, y2_predict)\n",
    "print('Error rate = %2.3f' % err_01loss)\n",
    "\n",
    "# Confusion matrix of prediction\n",
    "plot_conf_mat = PlotMetric(figsize=(4, 4))\n",
    "plot_conf_mat.set_labels(['Negative', 'Positive'])\n",
    "plot_conf_mat.confusion_matrix(y2, y2_predict, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4. Train with the Full Training Data and Submit to Kaggle\n",
    "----\n",
    "Now you've already have fine-tuned your `create_rnn`. Now train your model by leveraging full `X_train` and `y_train` dataset, predict the label of `X_test`, then submit to Kaggle.\n",
    "\n",
    "> **Your task: HW5.4.**\n",
    "> 1. Training with full data set `X_train` with the model created by `create_rnn`,\n",
    "> 2. Predict the **unknown** testing data `X_test` by the trained model, then\n",
    "> 3. Submit your result to Kaggle\n",
    ">\n",
    "> **Notice: You got 5 chances to submit your result every day.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and train\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "\n",
    "model = create_rnn()\n",
    "model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Predict the testing data\n",
    "y_test_categorical_predict = model.predict(X_test)\n",
    "y_test_predict = np.argmax(y_test_categorical_predict, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you submit\n",
    "----\n",
    "Please join the homework 5 competition by **using the Email ended with \\@trendmicro.com as your Kaggle InClass team name**.\n",
    "\n",
    "Type your Email in the variable `my_trendmicro_email_which_is_also_my_team_name` to make sure you've already read this paragraph, then the following code snippet will help you to generate the csv file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trendmicro_email_which_is_also_my_team_name = ''\n",
    "\n",
    "import re\n",
    "assert re.match(r\"[^@]+@trendmicro.com\", my_trendmicro_email_which_is_also_my_team_name), \"Please read the instruction above paragraph carefully\"\n",
    "\n",
    "target_path = 'data/hw05.result.csv'\n",
    "df_test_label = pd.DataFrame({'id': id_test, 'label': y_test_predict})\n",
    "df_test_label.to_csv(target_path, index=False)\n",
    "\n",
    "print('Congratulation! Please submit your result \\'%s\\' to https://www.kaggle.com/t/0d72d5a876864bc988259933bc35f3f2' % target_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
